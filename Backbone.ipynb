{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_oAYVDh8GPI"
   },
   "source": [
    "## Data Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ltruXuNCrbvS"
   },
   "outputs": [],
   "source": [
    "ppg_dir = '/Users/yeseullee/Documents/ECE271B/PPGBPDatabase/Data File/0_subject'\n",
    "label_path = '/Users/yeseullee/Documents/ECE271B/PPGBPDatabase/Data File/PPG-BP dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft-2is1YGx9v"
   },
   "source": [
    "## Choose High Quality Data\n",
    "(From the dataset description, it says every subject has 3 segments of ppg signals, and we need to choose the segment with highest quality for each subject.)\n",
    "(But for now, we just choose all segments as long as the quality is positive, so we can have more data to train.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Qm07sVE9Gvkq"
   },
   "outputs": [],
   "source": [
    "# organized data from https://www.nature.com/articles/sdata201820/tables/2\n",
    "\n",
    "quality_dict = dict()\n",
    "# key: subject number; value[0]: subject ID; value[1]:quality of segment 1; value[2]:quality of segment 2; value[3]:quality of segment 3\n",
    "quality_dict[1] = [2, 0.98, 0.96, 0.92]\n",
    "quality_dict[2] = [3, 0.69, 0.8, 0.81]\n",
    "quality_dict[3] = [6, 0.58, 0.59, 0.64]\n",
    "quality_dict[4] = [8, 0.96, 0.85, 0.87]\n",
    "quality_dict[5] = [9, 0.65, 0.67, 0.87]\n",
    "quality_dict[6] = [10, 0.59, 0.64, 0.34]\n",
    "quality_dict[7] = [11, 0.74, 0.67, -0.16]\n",
    "quality_dict[8] = [12, 0.23, 0.73, 0.41]\n",
    "quality_dict[9] = [13, 0.76, 0.84, 1.06]\n",
    "quality_dict[10] = [14, 0.77, 0.72, 0.15]\n",
    "\n",
    "quality_dict[11] = [15, 1.23, 0.77, 0.3]\n",
    "quality_dict[12] = [16, 0.64, 0.66, 0.83]\n",
    "quality_dict[13] = [17, 0.69, 0.9, 0.9]\n",
    "quality_dict[14] = [18, 0.87, 0.59, 1.05]\n",
    "quality_dict[15] = [19, 0.78, 0.19, 0.16]\n",
    "quality_dict[16] = [21, 0.65, 0.74, 0.75]\n",
    "quality_dict[17] = [22, 0.73, 0.44, 0.39]\n",
    "quality_dict[18] = [23, 0.7, 0.6, 0.74]\n",
    "quality_dict[19] = [24, 0.74, 0.74, 0.75]\n",
    "quality_dict[20] = [25, 1.38, 0.15, 0.78]\n",
    "\n",
    "quality_dict[21] = [26, 1.31, 0.47, 0.83]\n",
    "quality_dict[22] = [27, 1.86, 1.33, 1.26]\n",
    "quality_dict[23] = [29, 0.73, 0.6, -0.05]\n",
    "quality_dict[24] = [30, 0.86, 0.8, 0.76]\n",
    "quality_dict[25] = [31, 0.68, 0.81, 0.7]\n",
    "quality_dict[26] = [32, 1.06, 1.12, 1.23]\n",
    "quality_dict[27] = [34, 0.9, 0.79, 0.74]\n",
    "quality_dict[28] = [35, 0.94, 1.15, 1.18]\n",
    "quality_dict[29] = [38, 0.88, 0.66, 0.94]\n",
    "quality_dict[30] = [40, 0.21, 1.2, 0.9]\n",
    "\n",
    "quality_dict[31] = [41, 0.85, 0.78, 0.73]\n",
    "quality_dict[32] = [43, 0.52, 0.28, 0.53]\n",
    "quality_dict[33] = [45, 0.68, 0.76, 0.67]\n",
    "quality_dict[34] = [47, 0.76, 0.75, 0.73]\n",
    "quality_dict[35] = [48, 0.74, 0.79, 0.63]\n",
    "quality_dict[36] = [50, 0.79, 0.72, 0.73]\n",
    "quality_dict[37] = [51, 0.6, 0.2, 0.49]\n",
    "quality_dict[38] = [52, 0.95, 0.67, 0.93]\n",
    "quality_dict[39] = [53, 0.81, 0.91, 0.9]\n",
    "quality_dict[40] = [54, 1.12, 1.06, 1.11]\n",
    "\n",
    "quality_dict[41] = [55, 0.63, 0.99, 1.1]\n",
    "quality_dict[42] = [56, 0.26, 0.4, 0.47]\n",
    "quality_dict[43] = [57, 0.69, 0.59, 0.65]\n",
    "quality_dict[44] = [58, 0.97, 0.62, 0.75]\n",
    "quality_dict[45] = [60, 0.37, 1.64, 0.51]\n",
    "quality_dict[46] = [61, 0.87, 0.84, 0.93]\n",
    "quality_dict[47] = [62, 0.57, 0.96, 0.69]\n",
    "quality_dict[48] = [63, 1.01, 0.92, 0.92]\n",
    "quality_dict[49] = [64, 0.19, 0.65, 0.39]\n",
    "quality_dict[50] = [65, 0.78, 0.8, 0.72]\n",
    "\n",
    "quality_dict[51] = [66, 0.79, 0.79, 0.94]\n",
    "quality_dict[52] = [67, 0.7, 0.7, 0.99]\n",
    "quality_dict[53] = [83, 0.81, 0.9, 0.79]\n",
    "quality_dict[54] = [84, 0.91, 0.3, 0.65]\n",
    "quality_dict[55] = [85, 0.68, 0.79, 0.63]\n",
    "quality_dict[56] = [86, 0.66, 0.7, 0.72]\n",
    "quality_dict[57] = [87, 0.97, 0.96, 0.95]\n",
    "quality_dict[58] = [88, 0.81, 0.52, 0.65]\n",
    "quality_dict[59] = [89, 0.39, 0.58, 0.14]\n",
    "quality_dict[60] = [90, 0.87, 0.97, 1]\n",
    "\n",
    "quality_dict[61] = [91, 1.05, 0.77, 0.84]\n",
    "quality_dict[62] = [92, 0.9, 1.1, 1.15]\n",
    "quality_dict[63] = [93, 0.97, 0.99, 0.46]\n",
    "quality_dict[64] = [95, 1.31, 0.89, 0.87]\n",
    "quality_dict[65] = [96, 0.75, 0.89, 0.81]\n",
    "quality_dict[66] = [97, 0.56, 0.42, 0.76]\n",
    "quality_dict[67] = [98, 0.88, 0.98, 0.86]\n",
    "quality_dict[68] = [99, 0.88, 0.72, 0.79]\n",
    "quality_dict[69] = [100, 0.58, 0.66, 0.16]\n",
    "quality_dict[70] = [103, 0.37, 0.4, 0.44]\n",
    "\n",
    "quality_dict[71] = [104, 0.88, 0.23, 0.85]\n",
    "quality_dict[72] = [105, 0.54, 0.97, 0.8]\n",
    "quality_dict[73] = [106, 0.82, 0.9, 1.16]\n",
    "quality_dict[74] = [107, 0.89, 0.58, 0.66]\n",
    "quality_dict[75] = [108, 0.71, 0.69, 0.64]\n",
    "quality_dict[76] = [110, 0.9, 0.83, 0.88]\n",
    "quality_dict[77] = [111, 0.9, 0.85, 0.76]\n",
    "quality_dict[78] = [112, 0.61, 0.55, 0.57]\n",
    "quality_dict[79] = [113, 0.35, 0.51, 0.78]\n",
    "quality_dict[80] = [114, 0.5, 0.58, 0.67]\n",
    "\n",
    "quality_dict[81] = [115, 0.74, 0.06, 1.03]\n",
    "quality_dict[82] = [116, 0.78, 0.86, 0.93]\n",
    "quality_dict[83] = [119, 0.55, 0.59, -0.07]\n",
    "quality_dict[84] = [120, 0.79, 0.67, 0.77]\n",
    "quality_dict[85] = [122, 0.93, 0.87, 0.5]\n",
    "quality_dict[86] = [123, 0.89, 0.97, 1.3]\n",
    "quality_dict[87] = [124, 0.93, 1.23, 1.19]\n",
    "quality_dict[88] = [125, 0.84, -0.47, 0.86]\n",
    "quality_dict[89] = [126, 0.44, -0.01, 0.54]\n",
    "quality_dict[90] = [127, 0.53, 0.83, 0.75]\n",
    "\n",
    "quality_dict[91] = [128, 0.87, 0.86, 0.9]\n",
    "quality_dict[92] = [130, 0.91, 0.97, 1]\n",
    "quality_dict[93] = [131, 0.86, 0.85, 0.75]\n",
    "quality_dict[94] = [134, 0.71, 0.21, 0.86]\n",
    "quality_dict[95] = [135, 0.68, 0.72, 0.67]\n",
    "quality_dict[96] = [136, 1.73, 0.56, 0.8]\n",
    "quality_dict[97] = [137, 0.28, 0.58, 0.79]\n",
    "quality_dict[98] = [138, 0.74, 0.48, 0.59]\n",
    "quality_dict[99] = [139, 1.35, 0.69, 0.63]\n",
    "quality_dict[100] = [140, 0.41, 0.85, 0.71]\n",
    "\n",
    "quality_dict[101] = [141, 1.14, 0.96, 0.86]\n",
    "quality_dict[102] = [142, 0.8, 0.82, 0.83]\n",
    "quality_dict[103] = [144, 0.76, 0.66, -0.01]\n",
    "quality_dict[104] = [145, 1.11, 1.1, 1.1]\n",
    "quality_dict[105] = [146, 0.84, 0.84, 1]\n",
    "quality_dict[106] = [148, 1.03, 1.03, 1.06]\n",
    "quality_dict[107] = [149, 0.52, 0.58, 0.49]\n",
    "quality_dict[108] = [150, 0.75, 0.66, 0.49]\n",
    "quality_dict[109] = [151, 0.81, 0.29, 0.85]\n",
    "quality_dict[110] = [152, 1.05, 0.9, 1.22]\n",
    "\n",
    "quality_dict[111] = [153, 0.93, 1.15, 0.79]\n",
    "quality_dict[112] = [154, 0.82, 0.7, 0.75]\n",
    "quality_dict[113] = [155, 0.75, 0.94, 0.71]\n",
    "quality_dict[114] = [156, 0.75, 0.76, 0.7]\n",
    "quality_dict[115] = [157, 0.58, 0.68, 0.44]\n",
    "quality_dict[116] = [158, 0.66, 0.86, 0.05]\n",
    "quality_dict[117] = [160, 0.54, 0.66, 0.59]\n",
    "quality_dict[118] = [161, 0.89, 0.83, 0.86]\n",
    "quality_dict[119] = [162, 1.07, 1, 0.97]\n",
    "quality_dict[120] = [163, 0.79, 0.42, 0.61]\n",
    "\n",
    "quality_dict[121] = [164, 0.94, 0.85, 0.71]\n",
    "quality_dict[122] = [165, 1.07, 0.96, 1.02]\n",
    "quality_dict[123] = [166, 0.71, 0.93, 0.77]\n",
    "quality_dict[124] = [167, 0.57, 0.76, 0.57]\n",
    "quality_dict[125] = [169, 1.11, 0.82, 0.91]\n",
    "quality_dict[126] = [170, 0.77, 0.85, 0.95]\n",
    "quality_dict[127] = [171, 0.69, 0.46, 0.48]\n",
    "quality_dict[128] = [172, 0.45, 0.56, 0.53]\n",
    "quality_dict[129] = [173, 0.89, 0.84, 0.97]\n",
    "quality_dict[130] = [174, 1.14, 1.03, 1.17]\n",
    "\n",
    "quality_dict[131] = [175, 0.69, 0.62, 0.71]\n",
    "quality_dict[132] = [176, 0.75, 0.73, 0.68]\n",
    "quality_dict[133] = [178, 0.38, 0.68, 0.55]\n",
    "quality_dict[134] = [179, 2.34, 0.83, 0.78]\n",
    "quality_dict[135] = [180, 0.8, 0.64, 0.84]\n",
    "quality_dict[136] = [182, 0.72, 0.93, 0.9]\n",
    "quality_dict[137] = [183, 0.35, 0.25, 0.36]\n",
    "quality_dict[138] = [184, 0.49, 0.93, 0.87]\n",
    "quality_dict[139] = [185, 1.03, 1.1, 1.1]\n",
    "quality_dict[140] = [186, 0.73, 0.69, 0.99]\n",
    "\n",
    "quality_dict[141] = [188, 0.85, 1.78, 0.71]\n",
    "quality_dict[142] = [189, 0.9, 0.68, 0.92]\n",
    "quality_dict[143] = [190, 1.13, 0.8, 0.99]\n",
    "quality_dict[144] = [191, 1.23, 1.1, 0.85]\n",
    "quality_dict[145] = [192, 0.85, 0.87, 0.8]\n",
    "quality_dict[146] = [193, 0.76, 0.53, 0.63]\n",
    "quality_dict[147] = [195, 0.91, 1.1, 0.49]\n",
    "quality_dict[148] = [196, 0.88, 0.74, 0.68]\n",
    "quality_dict[149] = [197, 0.9, 1.06, 1.33]\n",
    "quality_dict[150] = [198, 1.12, 1.07, 1.02]\n",
    "\n",
    "quality_dict[151] = [199, 0.81, 0.96, 0.83]\n",
    "quality_dict[152] = [200, 0.3, 0.86, 1.02]\n",
    "quality_dict[153] = [201, 0.68, 0.69, 0.8]\n",
    "quality_dict[154] = [203, 0.92, 1.05, 0.86]\n",
    "quality_dict[155] = [205, 0.91, 0.82, 0.77]\n",
    "quality_dict[156] = [206, 0.69, 0.84, 0.67]\n",
    "quality_dict[157] = [207, 0.75, 0.7, 0.2]\n",
    "quality_dict[158] = [208, 1.47, 0.95, 0.92]\n",
    "quality_dict[159] = [209, 0.76, 0.67, 0.7]\n",
    "quality_dict[160] = [210, 0.72, 0.71, 0.78]\n",
    "\n",
    "quality_dict[161] = [211, 1.4, 0.73, 0.89]\n",
    "quality_dict[162] = [212, 0.74, 0.56, 0.59]\n",
    "quality_dict[163] = [213, 1.16, 0.91, 0.67]\n",
    "quality_dict[164] = [214, 0.46, 0.72, 0.24]\n",
    "quality_dict[165] = [215, 0.62, 0.69, 0.81]\n",
    "quality_dict[166] = [216, 0.33, 0.33, 0.37]\n",
    "quality_dict[167] = [217, 0.69, 1.26, 0.8]\n",
    "quality_dict[168] = [218, 0.82, 0.99, 0.89]\n",
    "quality_dict[169] = [219, 1.02, 1.05, 0.84]\n",
    "quality_dict[170] = [220, 0.63, 0.65, 0.66]\n",
    "\n",
    "quality_dict[171] = [221, 0.43, 0.78, 0.6]\n",
    "quality_dict[172] = [222, 0.92, 0.87, 0.85]\n",
    "quality_dict[173] = [223, 0.81, 0.08, 0.98]\n",
    "quality_dict[174] = [224, 0.85, 1.03, 0.75]\n",
    "quality_dict[175] = [226, 0.84, 1.04, 0.44]\n",
    "quality_dict[176] = [227, 0.99, 0.88, 0.94]\n",
    "quality_dict[177] = [228, 1.06, 1, 0.93]\n",
    "quality_dict[178] = [229, 1, 1.09, 0.98]\n",
    "quality_dict[179] = [230, 0.59, 0.72, 0.81]\n",
    "quality_dict[180] = [231, 1.28, 1.46, 0.98]\n",
    "\n",
    "quality_dict[181] = [232, 0.92, 1.21, 0.87]\n",
    "quality_dict[182] = [233, 0.89, 0.86, 0.67]\n",
    "quality_dict[183] = [234, 0.62, 0.81, 1.02]\n",
    "quality_dict[184] = [235, 0.94, 1.08, 0.97]\n",
    "quality_dict[185] = [237, 0.79, 1.19, 1.42]\n",
    "quality_dict[186] = [239, 0.81, 0.8, 0.7]\n",
    "quality_dict[187] = [240, 0.6, 1.09, 0.93]\n",
    "quality_dict[188] = [241, 0.68, 0.58, 0.52]\n",
    "quality_dict[189] = [242, 0.59, 0.7, 0.69]\n",
    "quality_dict[190] = [243, 1.09, 0.84, 0.91]\n",
    "\n",
    "quality_dict[191] = [244, 0.73, 0.79, 0.68]\n",
    "quality_dict[192] = [245, 0.56, 0.54, 7.13]\n",
    "quality_dict[193] = [246, 1.23, 1.85, 0.63]\n",
    "quality_dict[194] = [247, 0.8, 0.66, 0.54]\n",
    "quality_dict[195] = [248, 0.14, 0.65, 0.69]\n",
    "quality_dict[196] = [250, 0.79, 0.84, 0.77]\n",
    "quality_dict[197] = [251, 0.99, 0.95, 0.99]\n",
    "quality_dict[198] = [252, 0.78, 0.38, 10.22]\n",
    "quality_dict[199] = [253, 0.8, 0.89, 0.92]\n",
    "quality_dict[200] = [254, 0.51, 0.84, 0.75]\n",
    "\n",
    "quality_dict[201] = [256, 0.95, 0.72, 1.25]\n",
    "quality_dict[202] = [257, 0.63, 0.69, 0.87]\n",
    "quality_dict[203] = [259, 0.59, 0.62, 0.67]\n",
    "quality_dict[204] = [403, 0.92, 0.92, 0.92]\n",
    "quality_dict[205] = [404, 1.4, 1, 0.96]\n",
    "quality_dict[206] = [405, 0.72, 0.79, 0.96]\n",
    "quality_dict[207] = [406, 0.28, 0.45, 0.54]\n",
    "quality_dict[208] = [407, 0.84, 0.82, 0.76]\n",
    "quality_dict[209] = [409, 0.84, 1, 0.89]\n",
    "quality_dict[210] = [410, 0.94, 0.91, 0.9]\n",
    "\n",
    "quality_dict[211] = [411, 1.09, 0.92, 1.05]\n",
    "quality_dict[212] = [412, 0.95, 0.7, 0.99]\n",
    "quality_dict[213] = [413, 0.74, 0.67, 0.63]\n",
    "quality_dict[214] = [414, 0.93, 1.34, 1.4]\n",
    "quality_dict[215] = [415, 1.15, 1.38, 1.19]\n",
    "quality_dict[216] = [416, 0.96, 0.94, 1.01]\n",
    "quality_dict[217] = [417, 1.12, 1.32, 1.38]\n",
    "quality_dict[218] = [418, 0.96, 0.87, 1.06]\n",
    "quality_dict[219] = [419, 1.13, 1, 0.81]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFK4osfcgtgM"
   },
   "source": [
    "## Select all segments with positive quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOXh-KtLgcWj",
    "outputId": "7c5c9eda-b6be-4649-e459-bbab2c89f325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 3), (23, 3), (83, 3), (88, 2), (89, 2), (103, 3)]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# most of data have three segments, only few of them have some negative quality segments\n",
    "# -> just throw away those segments\n",
    "not_choose = []\n",
    "for subject, segment in quality_dict.items():\n",
    "  for seg, quality in enumerate(quality_dict[subject]):\n",
    "    if quality < 0:\n",
    "      not_choose.append((subject, seg))\n",
    "\n",
    "print(not_choose)\n",
    "print(len(not_choose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jd0EqNciCfj"
   },
   "source": [
    "## Set up PyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_wdg6a0xiB0I"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oeOfF_OIiI1K"
   },
   "outputs": [],
   "source": [
    "class BPDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, ppg_dir, label_path):\n",
    "\n",
    "        self.ppg_dir = ppg_dir\n",
    "        self.label_path = label_path\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        self.subjectid = []\n",
    "        \n",
    "        # read ppg data\n",
    "        for subject in range(1, 220):\n",
    "          subjectid = quality_dict[subject][0]\n",
    "          for segnum in range(1, 4):\n",
    "            if (subject, segnum) not in [(7, 3), (23, 3), (83, 3), (88, 2), (89, 2), (103, 3), (180, 1), (180, 2), (180, 3)]:\n",
    "              ppg_path = os.path.join(ppg_dir, '{}_{}.txt'.format(subjectid, segnum))\n",
    "              if os.path.exists(ppg_path):\n",
    "                with open(ppg_path) as f:\n",
    "                  lines = f.readlines()[0].split('\\t')[:-1]\n",
    "                  if len(lines) != 2100:\n",
    "                    print(subject, subjectid, segment, len(lines))\n",
    "                    continue\n",
    "                  ppg = torch.Tensor([float(x) for x in lines])\n",
    "                  self.data.append(ppg)\n",
    "                  self.subjectid.append(subjectid)\n",
    "        \n",
    "        # read BP labels (Label: 'Stage 1 hypertension' or 'Stage 2 hypertension'-> 2; 'Prehypertension'-> 1; 'Normal'-> 0)\n",
    "        df = pd.read_csv(label_path, skiprows=1)\n",
    "        print(set(df[\"Hypertension\"]))\n",
    "        for subject in range(219):\n",
    "          if df['Hypertension'][subject] == 'Stage 1 hypertension' or df['Hypertension'][subject] == 'Stage 2 hypertension':\n",
    "            bp_label = 2\n",
    "          elif df['Hypertension'][subject] == 'Prehypertension':\n",
    "            bp_label = 1\n",
    "          elif df['Hypertension'][subject] == 'Normal':\n",
    "            bp_label = 0\n",
    "          \n",
    "          if subject+1 == 180:\n",
    "            continue\n",
    "          elif subject+1 in [7, 23, 83, 88, 89, 103]:\n",
    "            self.label.extend([bp_label]*2)\n",
    "          else:\n",
    "            self.label.extend([bp_label]*3)\n",
    "        \n",
    "        self.label = torch.Tensor(self.label)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        subjectid = self.subjectid[index]\n",
    "        return data, label, subjectid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B5l3xGFv1Wz"
   },
   "source": [
    "## Setup Data loader and Split training, validation, testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SiEMsSueoFa7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzxhYUKLo98h",
    "outputId": "f88d855a-8dbc-4330-9746-2b69c3918718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Stage 1 hypertension', 'Normal', 'Stage 2 hypertension', 'Prehypertension'}\n",
      "dataset: 648\n",
      "415\n",
      "104\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "# setup data loader\n",
    "dataset = BPDataset(ppg_dir, label_path)\n",
    "#train_loader = data.DataLoader(dataset, shuffle=True, drop_last=False, pin_memory=True, batch_size=32)  # we can choose proper batch size\n",
    "print('dataset: {}'.format(dataset.__len__()))\n",
    "\n",
    "# Split training data, validation data, testing data\n",
    "data_train, data_val, data_test = torch.utils.data.random_split(dataset, [415, 104, 129])\n",
    "print(data_train.__len__())\n",
    "print(data_val.__len__())\n",
    "print(data_test.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes,device, num_epochs):\n",
    "    since = time.time()\n",
    "    model_later=copy.deepcopy(model)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "\n",
    "    train_acc=[]\n",
    "    val_acc=[]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            #running_cm=0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels, subject in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    loss=criterion(outputs,labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += (loss.item()) * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                #running_cm+=confusion_matrix(labels.cpu(), preds.cpu(),labels=[0,1])\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            #epoch_sen = (running_cm[1][1]/(running_cm[1][1]+running_cm[1][0]))\n",
    "            #epoch_pre = (running_cm[1][1]/(running_cm[1][1]+running_cm[0][1]))\n",
    "            #epoch_f1=(2*epoch_sen*epoch_pre)/(epoch_sen+epoch_pre)\n",
    "            #epoch_ppv=(running_cm[1][1]/(running_cm[1][1]+running_cm[0][1]))#(tp/(tp+fp))\n",
    "            \n",
    "            if phase=='train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_acc.append(np.float(epoch_acc))\n",
    "            else:\n",
    "                val_loss.append(epoch_loss)\n",
    "                val_acc.append(np.float(epoch_acc))\n",
    "\n",
    "            #print(phase,train_loss,val_loss)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            if phase == 'val' and epoch_acc >= best_acc:\n",
    "                #best_acc = epoch_acc\n",
    "                best_model_wts_later = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc+Sen: {:4f}'.format(best_acc))\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "    fig, axs=plt.subplots(2)\n",
    "    axs[0].set_title('model loss')\n",
    "    axs[1].set_title('model accuracy')\n",
    "    for ax in axs.flat:\n",
    "        ax.set_ylim([0.0,1.0])\n",
    "    axs[0].plot(train_loss,'r',val_loss,'g',)\n",
    "    axs[1].plot(train_acc,'r',val_acc,'g')\n",
    "    fig.tight_layout()\n",
    "    for ax in axs.flat:\n",
    "        leg=ax.legend(['train','val'])\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    model_later.load_state_dict(best_model_wts_later)\n",
    "    \n",
    "    return model,model_later\n",
    "\n",
    "def test_model(model,dataloaders,dataset_sizes,device):\n",
    "    \n",
    "    model.eval()\n",
    "    corrects=0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['test']:\n",
    "            images, labels, subject= data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images) \n",
    "            preds = torch.argmax(outputs.data, 1)\n",
    "        \n",
    "            corrects += torch.sum(preds == labels.data)\n",
    "        acc=corrects.double() / dataset_sizes[phase]\n",
    "        \n",
    "        print('\\nTestset Accuracy(mean): %f %%' % (100 * acc))\n",
    "                \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 64)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResBlock(64, 128, stride=2, downsample=nn.Sequential(\n",
    "                nn.Conv1d(64, 128, kernel_size=1, stride=2, bias=False),\n",
    "                nn.BatchNorm1d(128)\n",
    "            )),\n",
    "            ResBlock(128, 128)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResBlock(128, 256, stride=2, downsample=nn.Sequential(\n",
    "                nn.Conv1d(128, 256, kernel_size=1, stride=2, bias=False),\n",
    "                nn.BatchNorm1d(256)\n",
    "            )),\n",
    "            ResBlock(256, 256)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ResBlock(256, 512, stride=2, downsample=nn.Sequential(\n",
    "                nn.Conv1d(256, 512, kernel_size=1, stride=2, bias=False),\n",
    "                nn.BatchNorm1d(512)\n",
    "            )),\n",
    "            ResBlock(512, 512)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VGG11(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv5 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        self.conv6 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.conv7 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv8 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn8 = nn.BatchNorm1d(512)\n",
    "        self.conv9 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn9 = nn.BatchNorm1d(512)\n",
    "        self.conv10 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn10 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv11 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm1d(512)\n",
    "        self.conv12 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm1d(512)\n",
    "        self.conv13 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn13 = nn.BatchNorm1d(512)\n",
    "        self.pool5 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        '''self.fc1 = nn.Linear(512, 4096)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)'''\n",
    "        \n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv8(x)\n",
    "        x = self.bn8(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.bn9(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.bn10(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.conv11(x)\n",
    "        x = self.bn11(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.bn12(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.bn13(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
